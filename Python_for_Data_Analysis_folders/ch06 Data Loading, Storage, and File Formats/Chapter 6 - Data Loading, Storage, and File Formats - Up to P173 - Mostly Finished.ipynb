{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6 - Data Loading, Storage, and File Formats\n",
    "\n",
    "The tools in this book are of little use if you can’t easily import and export data in\n",
    "Python. I’m going to be focused on input and output with pandas objects, though there\n",
    "are of course numerous tools in other libraries to aid in this process. NumPy, for example,\n",
    "features low-level but extremely fast binary data loading and storage, including\n",
    "support for memory-mapped array. See Chapter 12 for more on those.\n",
    "Input and output typically falls into a few main categories: reading text files and other\n",
    "more efficient on-disk formats, loading data from databases, and interacting with network\n",
    "sources like web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data in Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has become a beloved language for text and file munging due to its simple syntax\n",
    "for interacting with files, intuitive data structures, and convenient features like tuple\n",
    "packing and unpacking.\n",
    "\n",
    "pandas features a number of functions for reading tabular data as a DataFrame object.\n",
    "Table 6-1 has a summary of all of them, though read_csv and read_table are likely the\n",
    "ones you’ll use the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:24.931911",
     "start_time": "2017-06-07T12:32:24.926156"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Table 6-1. Parsing functions in pandas\n",
    "\n",
    "#Function           Description\n",
    "\n",
    "#read_csv           Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\n",
    "#read_table         Load delimited data from a file, URL, or file-like object. Use tab ('\\t') as default delimiter\n",
    "#read_fwf           Read data in fixed-width column format (that is, no delimiters)\n",
    "#read_clipboard     Version of read_table that reads data from the clipboard. Useful for converting tables from web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I’ll give an overview of the mechanics of these functions, which are meant to convert\n",
    "text data into a DataFrame. The options for these functions fall into a few categories:\n",
    "\n",
    "• Indexing: can treat one or more columns as the returned DataFrame, and whether\n",
    "to get column names from the file, the user, or not at all.\n",
    "\n",
    "• Type inference and data conversion: this includes the user-defined value conversions\n",
    "and custom list of missing value markers.\n",
    "\n",
    "• Datetime parsing: includes combining capability, including combining date and\n",
    "time information spread over multiple columns into a single column in the result.\n",
    "\n",
    "• Iterating: support for iterating over chunks of very large files.\n",
    "\n",
    "• Unclean data issues: skipping rows or a footer, comments, or other minor things\n",
    "like numeric data with thousands separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:24.945614",
     "start_time": "2017-06-07T12:32:24.936931"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type inference is one of the more important features of these functions; that means you\n",
    "# don’t have to specify which columns are numeric, integer, boolean, or string. Handling\n",
    "# dates and other custom types requires a bit more effort, though. Let’s start with a small\n",
    "# comma-separated (CSV) text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.443070",
     "start_time": "2017-06-07T12:32:24.946998"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.468574",
     "start_time": "2017-06-07T12:32:25.446677"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   d message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since this is comma-delimited, we can use read_csv to read it into a DataFrame:\n",
    "\n",
    "df = pd.read_csv('Chapter 6 practice csv.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.759684",
     "start_time": "2017-06-07T12:32:25.473806"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withoutheader.csv: not found\r\n"
     ]
    }
   ],
   "source": [
    "# Here I used the Unix cat shell command to print the raw contents of\n",
    "#the file to the screen. If you’re on Windows, you can use type instead\n",
    "#of cat to achieve the same effect. Page 156\n",
    "\n",
    "!type withoutheader.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.778504",
     "start_time": "2017-06-07T12:32:25.762783"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   d message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We could also have used read_table and specifying the delimiter:\n",
    "pd.read_table('Chapter 6 practice csv.csv', sep = ',')\n",
    "# here we see no difference since this file is already a csv file, meaning comma seperated already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.898089",
     "start_time": "2017-06-07T12:32:25.781886"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'withoutheader.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-632671d1e96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#To read this in, you have a couple of options. You can allow pandas to assign default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#column names, or you can specify names yourself:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'withoutheader.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'withoutheader.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#To read this in, you have a couple of options. You can allow pandas to assign default\n",
    "#column names, or you can specify names yourself:\n",
    "pd.read_csv('withoutheader.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:54.940270",
     "start_time": "2017-06-07T12:32:54.914849"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'withoutheader.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-75574f469e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'withoutheader.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'withoutheader.csv' does not exist"
     ]
    }
   ],
   "source": [
    "pd.read_csv('withoutheader.csv', names = ['a','b','c','d','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:59.142028",
     "start_time": "2017-06-07T12:32:59.138888"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppose you wanted the message column to be the index of the returned DataFrame.\n",
    "# You can either indicate you want the column at index 4 or named 'message' using the\n",
    "# index_col argument:\n",
    "\n",
    "names = ['a','b','c','d','message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:33:02.041719",
     "start_time": "2017-06-07T12:33:02.018746"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'withoutheader.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ddd98d0e2404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'withoutheader.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/user/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'withoutheader.csv' does not exist"
     ]
    }
   ],
   "source": [
    "pd.read_csv('withoutheader.csv', names = names, index_col = 'message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.899772",
     "start_time": "2017-06-07T16:32:43.962Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the event that you want to form a hierarchical index from multiple columns, just\n",
    "# pass a list of column numbers or names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.899963",
     "start_time": "2017-06-07T16:32:43.964Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to create a csv file\n",
    "# This part is added by myself to get familiar with creating a file.\n",
    "import csv\n",
    "with open('test.csv', 'w', newline = '') as csvfile:\n",
    "    a = csv.writer(csvfile, delimiter=',')\n",
    "    data = [['Stock','Sales'],\n",
    "           ['100','24'],\n",
    "           ['120','33'],\n",
    "            ['23','5']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.900195",
     "start_time": "2017-06-07T16:32:43.967Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use DataFrame to display\n",
    "dt = DataFrame(data)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.900387",
     "start_time": "2017-06-07T16:32:43.978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try my own real-life example\n",
    "pd.read_csv('SPY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.900584",
     "start_time": "2017-06-07T16:32:43.981Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's use this example to try applications from the textbook - starting from Page 157\n",
    "\n",
    "# In the event that you want to form a hierarchical index from multiple columns, just\n",
    "# pass a list of column numbers or names:\n",
    "\n",
    "!type SPY.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.900770",
     "start_time": "2017-06-07T16:32:43.993Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a hierarchical index\n",
    "parsed = pd.read_csv('SPY.csv', index_col = ['Date', 'Open',])\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.900959",
     "start_time": "2017-06-07T16:32:43.997Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In some cases, a table might not have a fixed delimiter, using whitespace or some other\n",
    "# pattern to separate fields. In these cases, you can pass a regular expression as a delimiter\n",
    "# for read_table. Consider a text file that looks like this:\n",
    "list(open('SPY.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.901190",
     "start_time": "2017-06-07T16:32:44.003Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# While you could do some munging by hand, in this case fields are separated by a variable\n",
    "# amount of whitespace. This can be expressed by the regular expression \\s+, so we have\n",
    "# then:\n",
    "\n",
    "result = pd.read_table('SPY.csv', sep = '\\s+')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because there was one fewer column name than the number of data rows, read_table infers that the first column should be the DataFrame’s index in this special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser functions have many additional arguments to help you handle the wide variety of exception file formats that occur (see Table 6-2). For example, you can skip the first, third, and fourth rows of a file with skiprows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.901378",
     "start_time": "2017-06-07T16:32:44.017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('SPY.csv',skiprows = [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.901562",
     "start_time": "2017-06-07T16:32:44.022Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handling missing values is an important and frequently nuanced part of the file parsing\n",
    "# process. Missing data is usually either not present (empty string) or marked by some\n",
    "# sentinel value. By default, pandas uses a set of commonly occurring sentinels, such as NA, -1.#IND, and NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.901742",
     "start_time": "2017-06-07T16:32:44.026Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!type US3M.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.901922",
     "start_time": "2017-06-07T16:32:44.030Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv('US3M.csv')\n",
    "pd.isnull(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.902184",
     "start_time": "2017-06-07T16:32:44.037Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The na_values option can take either a list or set of strings to consider missing values:\n",
    "result = pd.read_csv('US3M.csv', na_values = ['NULL'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.902374",
     "start_time": "2017-06-07T16:32:44.040Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different NA sentinels can be specified for each column in a dict:\n",
    "sentinels = {'message': ['foo', 'NA'], 'something':['two']}\n",
    "sentinels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.902555",
     "start_time": "2017-06-07T16:32:44.045Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt1 = pd.read_csv('US3M.csv', na_values = sentinels)\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.902739",
     "start_time": "2017-06-07T16:32:44.048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doing a little cleasing in my way\n",
    "clean_values = dt1.replace({'.': 0.0001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument Description\n",
    "\n",
    "path String indicating filesystem location, URL, or file-like object\n",
    "\n",
    "sep or delimiter Character sequence or regular expression to use to split fields in each row\n",
    "\n",
    "header Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header\n",
    "row\n",
    "\n",
    "\n",
    "index_col Column numbers or names to use as the row index in the result. Can be a single name/number or a list\n",
    "of them for a hierarchical index\n",
    "\n",
    "names List of column names for result, combine with header=None\n",
    "\n",
    "skiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\n",
    "\n",
    "na_values Sequence of values to replace with NA\n",
    "\n",
    "comment Character or characters to split comments off the end of lines\n",
    "\n",
    "parse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise\n",
    "can specify a list of column numbers or name to parse. If element of list is tuple or list, will combine\n",
    "multiple columns together and parse to date (for example if date/time split across two columns)\n",
    "\n",
    "keep_date_col If joining columns to parse date, drop the joined columns. Default True\n",
    "\n",
    "converters Dict containing column number of name mapping to functions. For example {'foo': f} would apply\n",
    "the function f to all values in the 'foo' column\n",
    "\n",
    "dayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,\n",
    "2012). Default False\n",
    "\n",
    "date_parser Function to use to parse dates\n",
    "\n",
    "nrows Number of rows to read from beginning of file\n",
    "\n",
    "iterator Return a TextParser object for reading file piecemeal\n",
    "\n",
    "chunksize For iteration, size of file chunks\n",
    "\n",
    "skip_footer Number of lines to ignore at end of file\n",
    "\n",
    "verbose Print various parser output information, like the number of missing values placed in non-numeric\n",
    "columns\n",
    "\n",
    "encoding Text encoding for unicode. For example 'utf-8' for UTF-8 encoded text\n",
    "\n",
    "squeeze If the parsed data only contains one column return a Series\n",
    "\n",
    "thousands Separator for thousands, e.g. ',' or '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.902919",
     "start_time": "2017-06-07T16:32:44.057Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When processing very large files or figuring out the right set of arguments to correctly\n",
    "# process a large file, you may only want to read in a small piece of a file or iterate through\n",
    "# smaller chunks of the file.\n",
    "clean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.903166",
     "start_time": "2017-06-07T16:32:44.062Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want to only read out a small number of rows (avoiding reading the entire file),\n",
    "# specify that with nrows:\n",
    "pd.read_csv('US3M.csv', nrows = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.903348",
     "start_time": "2017-06-07T16:32:44.065Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunker = pd.read_csv('US3M.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.903525",
     "start_time": "2017-06-07T16:32:44.068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The TextParser object returned by read_csv allows you to iterate over the parts of the\n",
    "#file according to the chunksize. For example, we can iterate over ex6.csv, aggregating\n",
    "#the value counts in the 'key' column like so:\n",
    "tot = Series([])\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['USD3MTD156N'].value_counts(), fill_value = 0)\n",
    "    \n",
    "tot  = tot.sort_values(ascending = False)\n",
    "\n",
    "tot[:10]\n",
    "#not working, will get back to this with real projects so will understand what it really means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data Out to Text Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.903703",
     "start_time": "2017-06-07T16:32:44.071Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data can also be exported to delimited format. Let’s consider one of the CSV files read above:\n",
    "data = pd.read_csv('US3M.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.903880",
     "start_time": "2017-06-07T16:32:44.079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.904140",
     "start_time": "2017-06-07T16:32:44.081Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using DataFrame’s to_csv method, we can write the data out to a comma-separated file:\n",
    "data.to_csv('outdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.904329",
     "start_time": "2017-06-07T16:32:44.085Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!type outdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.904507",
     "start_time": "2017-06-07T16:32:44.089Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other delimiters cab be used, of course (writing to ... so it just prints the text results.):\n",
    "data.to_csv('outdata.csv', sep = '|')\n",
    "!type outdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.904680",
     "start_time": "2017-06-07T16:32:44.092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Missing values apperar as emplty strings in the output. You might want to denote them by some other sentinels values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.904858",
     "start_time": "2017-06-07T16:32:44.094Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('outdata.csv', na_rep= \"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.905141",
     "start_time": "2017-06-07T16:32:44.097Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!type outdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.905392",
     "start_time": "2017-06-07T16:32:44.102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With no other options specificed, both the row and the column labels are written. \n",
    "# Both of these can be disables.\n",
    "data.to_csv('outdata.csv', index = False, header = False)\n",
    "!type outdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.905570",
     "start_time": "2017-06-07T16:32:44.105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also write only a subset of the columns, and in an order of your choosing\n",
    "data.to_csv('outdata.csv', index = False, columns = ['DATE','USD3MTD156N'])\n",
    "\n",
    "!type outdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.905747",
     "start_time": "2017-06-07T16:32:44.110Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series also has a to_csv method:\n",
    "from datetime import datetime\n",
    "dates = pd.date_range('1/1/2000', period = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.905921",
     "start_time": "2017-06-07T16:32:44.112Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = Series(np.arange(7), index = dates)\n",
    "ts.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.906354",
     "start_time": "2017-06-07T16:32:44.115Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With a bit of wrangling (no header, first column as index), you can read a CSV version\n",
    "# of a Series with read_csv, but there is also a from_csv convenience method that makes\n",
    "# it a bit simpler:\n",
    "Series.from_csv('outdata.csv', parse_dates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Working with Delimited Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.906541",
     "start_time": "2017-06-07T16:32:44.118Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Most forms of tabular data can be loaded from disk using functions like pan\n",
    "#das.read_table. In some cases, however, some manual processing may be necessary.\n",
    "#It’s not uncommon to receive a file with one or more malformed lines that trip up\n",
    "#read_table. To illustrate the basic tools, consider a small CSV file:\n",
    "\n",
    "!type ex7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.906723",
     "start_time": "2017-06-07T16:32:44.121Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For any file with a single-character delimiter, you can use Python’s built-in csv module.\n",
    "#To use it, pass any open file or file-like object to csv.reader:\n",
    "import csv\n",
    "f = open('ex7.csv')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.906903",
     "start_time": "2017-06-07T16:32:44.124Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can only use 'line' here.\n",
    "for line in reader:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.907158",
     "start_time": "2017-06-07T16:32:44.127Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From there, it’s up to you to do the wrangling necessary to put the data in the form\n",
    "# that you need it. For example:\n",
    "lines = list(csv.reader(open('ex7.csv')))\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.907342",
     "start_time": "2017-06-07T16:32:44.131Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header, values = lines[0], lines[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.907519",
     "start_time": "2017-06-07T16:32:44.135Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.907691",
     "start_time": "2017-06-07T16:32:44.137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.907871",
     "start_time": "2017-06-07T16:32:44.143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QUESTION: Can generate the result since v and h are not defined? \n",
    "data_dict = {h: v for h, bv in zip(header, zip(*values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.908205",
     "start_time": "2017-06-07T16:32:44.147Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CSV files come in many different flavors. Defining a new format with a different delimiter,\n",
    "# string quoting convention, or line terminator is done by defining a simple subclass\n",
    "# of csv.Dialect:\n",
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "reader = csv.reader(f, dialect = my_dialect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.908430",
     "start_time": "2017-06-07T16:32:44.150Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Individual CSV dialect parameters can also be given as keywords to csv.reader without\n",
    "# having to define a subclass:\n",
    "reader = csv.reader(f, delimiter = '|')\n",
    "#My own practice:\n",
    "pd.read_csv('ex7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible options (attributes of csv.Dialect) and what they do can be found in\n",
    "Table 6-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.908684",
     "start_time": "2017-06-07T16:32:44.153Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Table 6-3. CSV dialect options\n",
    "\n",
    "#Argument               Description\n",
    "#delimiter              One-character string to separate fields. Defaults to ','.\n",
    "#lineterminator         Line terminator for writing, defaults to '\\r\\n'. Reader ignores this and recognizes\n",
    "#                       cross-platform line terminators.\n",
    "#quotechar              Quote character for fields with special characters (like a delimiter). Default is '\"'.\n",
    "#quoting                Quoting convention. Options include csv.QUOTE_ALL (quote all fields),\n",
    "#                       csv.QUOTE_MINIMAL (only fields with special characters like the delimiter),\n",
    "#                       csv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s\n",
    "#                       documentation for full details. Defaults to QUOTE_MINIMAL.\n",
    "# skipinitialspace      Ignore whitespace after each delimiter. Default False.\n",
    "# doublequote           How to handle quoting character inside a field. If True, it is doubled. See online\n",
    "#                       documentation for full detail and behavior.\n",
    "# escapechar            String to escape the delimiter if quoting is set to csv.QUOTE_NONE. Disabled by\n",
    "#                       default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For files with more complicated or fixed multicharacter delimiters, you\n",
    "will not be able to use the csv module. In those cases, you’ll have to do\n",
    "the line splitting and other cleanup using string’s split method or the\n",
    "regular expression method re.split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.908884",
     "start_time": "2017-06-07T16:32:44.157Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To write delimited files manually, you can use csv.writer. It accepts an open, writable\n",
    "# file object and the same dialect and format options as csv.reader:\n",
    "with open('mydata.csv', 'w') as f:\n",
    "    writer = csv.writer(f, dialect=my_dialect)\n",
    "    writer.writerow(('one', 'two', 'three'))\n",
    "    writer.writerow(('1', '2', '3'))\n",
    "    writer.writerow(('4', '5', '6'))\n",
    "    writer.writerow(('7', '8', '9'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.909144",
     "start_time": "2017-06-07T16:32:44.160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#JSON (short for JavaScript Object Notation) has become one of the standard formats\n",
    "#for sending data by HTTP request between web browsers and other applications. It is\n",
    "#a much more flexible data format than a tabular text form like CSV. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.909340",
     "start_time": "2017-06-07T16:32:44.163Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\":\"Wes\",\n",
    "\"places_lived\":[\"United States\", \"Spain\", \"Germany\"],\n",
    "\"pet\": null,\n",
    "\"siblings\":[{\"name\":\"Scott\", \"age\": 25, \"pet\":\"Zuko\"},\n",
    "            {\"name\":\"Katie\", \"age\": 33, \"pet\":\"Cisco\"}]}\n",
    "            \"\"\"\n",
    "\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is very nearly valid Python code with the exception of its null value null and\n",
    "some other nuances (such as disallowing trailing commas at the end of lists). The basic\n",
    "types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All of the\n",
    "keys in an object must be strings. There are several Python libraries for reading and\n",
    "writing JSON data. I’ll use json here as it is built into the Python standard library. To\n",
    "convert a JSON string to Python form, use json.loads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.909519",
     "start_time": "2017-06-07T16:32:44.165Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert a JSON string to Python form\n",
    "import json\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json.dumps on the other hand converts a Python object back to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.909693",
     "start_time": "2017-06-07T16:32:44.169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asjson = json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you convert a JSON object or list of objects to a DataFrame or some other data\n",
    "structure for analysis will be up to you. Conveniently, you can pass a list of JSON objects\n",
    "to the DataFrame constructor and select a subset of the data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.909867",
     "start_time": "2017-06-07T16:32:44.172Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siblings = DataFrame(result['siblings'], columns = ['name', 'age', 'pet'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extended example of reading and manipulating JSON data (including nested\n",
    "records), see the USDA Food Database example in the next chapter.\n",
    "\n",
    "(An effort is underway to add fast native JSON export (to_json) and\n",
    "decoding (from_json) to pandas. This was not ready at the time of writing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML and HTML: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has many libraries for reading and writing data in the ubiquitous HTML and\n",
    "XML formats. lxml (http://lxml.de) is one that has consistently strong performance in\n",
    "parsing very large files. lxml has multiple programmer interfaces; first I’ll show using\n",
    "lxml.html for HTML, then parse some XML using lxml.objectify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many websites make data available in HTML tables for viewing in a browser, but not\n",
    "downloadable as an easily machine-readable format like JSON, HTML, or XML. I noticed\n",
    "that this was the case with Yahoo! Finance’s stock options data. If you aren’t\n",
    "familiar with this data; options are derivative contracts giving you the right to buy\n",
    "(call option) or sell (put option) a company’s stock at some particular price (the\n",
    "strike) between now and some fixed point in the future (the expiry). People trade both\n",
    "call and put options across many strikes and expiries; this data can all be found together\n",
    "in tables on Yahoo! Finance.\n",
    "\n",
    "To get started, find the URL you want to extract data from, open it with urllib2 and\n",
    "parse the stream with lxml like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.910169",
     "start_time": "2017-06-07T16:32:44.175Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.910351",
     "start_time": "2017-06-07T16:32:44.178Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))\n",
    "doc = parsed.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using this object, you can extract all HTML tags of a particular type, such as table tags\n",
    "containing the data of interest. As a simple motivating example, suppose you wanted\n",
    "to get a list of every URL linked to in the document; links are a tags in HTML. Using\n",
    "the document root’s findall method along with an XPath (a means of expressing\n",
    "“queries” on the document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.910523",
     "start_time": "2017-06-07T16:32:44.180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = doc.findall('.//a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.910701",
     "start_time": "2017-06-07T16:32:44.184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links[15:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "But these are objects representing HTML elements; to get the URL and link text you\n",
    "have to use each element’s get method (for the URL) and text_content method (for\n",
    "the display text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.910952",
     "start_time": "2017-06-07T16:32:44.187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk = links[25]\n",
    "lnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.911475",
     "start_time": "2017-06-07T16:32:44.189Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.911957",
     "start_time": "2017-06-07T16:32:44.194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk.text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, getting a list of all URLs in the document is a matter of writing this list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.912324",
     "start_time": "2017-06-07T16:32:44.198Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = [lnk.get('href') for lnk in doc.findall('.//a')]\n",
    "urls[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finding the right tables in the document can be a matter of TRIAL AND ERROR; some\n",
    "websites make it easier by giving a table of interest an id attribute. I determined that\n",
    "these were the two tables containing the call data and put data, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.912626",
     "start_time": "2017-06-07T16:32:44.201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = doc.findall('.//table')\n",
    "calls = tables[0]\n",
    "puts = tables[0]\n",
    "\n",
    "calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.913036",
     "start_time": "2017-06-07T16:32:44.204Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "puts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each table has a header row followed by each of the data rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.913265",
     "start_time": "2017-06-07T16:32:44.207Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = calls.findall('.//tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the header as well as the data rows, we want to extract the text from each cell; in\n",
    "the case of the header these are th cells and td cells for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.913516",
     "start_time": "2017-06-07T16:32:44.210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _unpack(row, kind = 'td'):\n",
    "    elts = row.findall('.//%s' % kind)\n",
    "    return [val.text_content() for val in elts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.913707",
     "start_time": "2017-06-07T16:32:44.213Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_unpack(rows[0], kind = 'th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.913941",
     "start_time": "2017-06-07T16:32:44.215Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_unpack(rows[0], kind='td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it’s a matter of combining all of these steps together to convert this data into a\n",
    "DataFrame. Since the numerical data is still in string format, we want to convert some,\n",
    "but perhaps not all of the columns to floating point format. You could do this by hand,\n",
    "but, luckily, pandas has a class TextParser that is used internally in the read_csv and\n",
    "other parsing functions to do the appropriate automatic type conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.914148",
     "start_time": "2017-06-07T16:32:44.220Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.parsers import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.914327",
     "start_time": "2017-06-07T16:32:44.224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_options_data(table):\n",
    "    rows = table.findall('.//tr')\n",
    "    header = _unpack(rows[0], kind = 'th')\n",
    "    data = [_unpack(r) for r in rows[1:]]\n",
    "    return TextParser(data, names = header).get_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we invoke this parsing function on the lxml table objects and get DataFrame\n",
    "results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.914504",
     "start_time": "2017-06-07T16:32:44.228Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#QUESTION: Gotta figure this out..\n",
    "call_data = parse_options_data(calls)\n",
    "put_data = parse_options_data(puts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bonus: Fetching the Yahoo Finance Page \n",
    "http://pythoncentral.io/python-beautiful-soup-example-yahoo-finance-scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.914678",
     "start_time": "2017-06-07T16:32:44.231Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optionUrl = 'http://finance.yahoo.com/q/op?s=AAPL+Options'\n",
    "optionsPage = urlopen(optionUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This code retrieves the Yahoo Finance HTML and returns a file-like object.\n",
    "\n",
    "If you go to the page we opened with Python and use your browser's \"get source\" command you'll see that it's a large, complicated HTML file. It will be Python's job to simplify and extract the useful data using the BeautifulSoup module. BeautifulSoup is an external module so you'll have to install it. If you haven't installed BeautifulSoup already, you can get it here\n",
    "http://pythoncentral.io/python-beautiful-soup-example-yahoo-finance-scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.914882",
     "start_time": "2017-06-07T16:32:44.235Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beautiful Soup Example: Loading a Page\n",
    "# The following code will load the page into BeautifulSoup:\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(optionsPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.915071",
     "start_time": "2017-06-07T16:32:44.238Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.findAll(text = 'AAPL161230C00117000')[0].parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This result isn’t very useful yet. It’s just a unicode string (that's what the 'u' means) of what we searched for. However BeautifulSoup returns things in a tree format so we can find the context in which this text occurs by asking for it's parent node like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.915248",
     "start_time": "2017-06-07T16:32:44.241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo. It's still a little messy, but you can see all of the data that we need is there. If you ignore all the stuff in brackets, you can see that this is just the data from one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.915424",
     "start_time": "2017-06-07T16:32:44.243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optionsTable = [\n",
    "    [x.text for x in y.parent.contents]\n",
    "    for y in soup.findAll('td', attrs={'class': 'yfnc_h', 'nowrap': ''})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.915600",
     "start_time": "2017-06-07T16:32:44.246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optionsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This code is a little dense, so let's take it apart piece by piece. The code is a list comprehension within a list comprehension. Let's look at the inner one first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.915773",
     "start_time": "2017-06-07T16:32:44.249Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for y in soup.findAll('td', attrs={'class': 'yfnc_h', 'nowrap': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This uses BeautifulSoup's findAll function to get all of the HTML elements with a td tag, a class of yfnc_h and a nowrap of nowrap. We chose this because it's a unique element in every table entry.\n",
    "\n",
    "If we had just gotten td's with the class yfnc_h we would have gotten seven elements per table entry. Another thing to note is that we have to wrap the attributes in a dictionary because class is one of Python's reserved words. From the table above it would return this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<td nowrap=\"nowrap\"><a href=\"/q/op?s=AAPL&amp;amp;k=110.000000\"><strong>110.00</strong></a></td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get one level higher and then get the text from all of the child nodes of this node's parent. That's what this code does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.916071",
     "start_time": "2017-06-07T16:32:44.254Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x.text for x in y.parent.contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but you should be careful if this is code you plan to frequently reuse. If Yahoo changed the way they format their HTML, this could stop working. If you plan to use code like this in an automated way it would be best to wrap it in a try/catch block and validate the output.\n",
    "\n",
    "This is only a simple Beautiful Soup example, and gives you an idea of what you can do with HTML and XML parsing in Python. You can find the Beautiful Soup documentation here. You'll find a lot more tools for searching and validating HTML documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Parsing XML with lxml.objectify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "XML (extensible markup language) is another common structured data format supporting\n",
    "hierarchical, nested data with metadata. The files that generate the book you\n",
    "are reading actually form a series of large XML documents.\n",
    "Above, I showed the lxml library and its lxml.html interface. Here I show an alternate\n",
    "interface that’s convenient for XML data, lxml.objectify.\n",
    "The New York Metropolitan Transportation Authority (MTA) publishes a number of\n",
    "data series about its bus and train services (http://www.mta.info/developers/download\n",
    ".html). Here we’ll look at the performance data which is contained in a set of XML files.\n",
    "Each train or bus service has a different file (like Performance_MNR.xml for the Metro-\n",
    "North Railroad) containing monthly data as a series of XML records that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<INDICATOR>\n",
    "    <INDICATOR_SEQ>373889</INDICATOR_SEQ>\n",
    "    <PARENT_SEQ></PARENT_SEQ>\n",
    "    <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\n",
    "    <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\n",
    "    <DESCRIPTION>Percent of the time that escalators are operational\n",
    "    systemwide. The availability rate is based on physical observations performed\n",
    "    the morning of regular business days only. This is a new indicator the agency\n",
    "    began reporting in 2009.</DESCRIPTION>\n",
    "    <PERIOD_YEAR>2011</PERIOD_YEAR>\n",
    "    <PERIOD_MONTH>12</PERIOD_MONTH>\n",
    "    <CATEGORY>Service Indicators</CATEGORY>\n",
    "    <FREQUENCY>M</FREQUENCY>\n",
    "    <DESIRED_CHANGE>U</DESIRED_CHANGE>\n",
    "    <INDICATOR_UNIT>%</INDICATOR_UNIT>\n",
    "    <DECIMAL_PLACES>1</DECIMAL_PLACES>\n",
    "    <YTD_TARGET>97.00</YTD_TARGET>\n",
    "    <YTD_ACTUAL></YTD_ACTUAL>\n",
    "    <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\n",
    "    <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\n",
    "</INDICATOR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using lxml.objectify, we parse the file and get a reference to the root node of the XML\n",
    "file with getroot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.916247",
     "start_time": "2017-06-07T16:32:44.259Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import objectify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.916500",
     "start_time": "2017-06-07T16:32:44.262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'Performance_MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "root.INDICATOR return a generator yielding each <INDICATOR> XML element. For each\n",
    "record, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding\n",
    "a few tags):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the easiest ways to store data efficiently in binary format is using Python’s builtin\n",
    "pickle serialization. Conveniently, pandas objects all have a save method which\n",
    "writes the data to disk as a pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.916692",
     "start_time": "2017-06-07T16:32:44.266Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv('US3M.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T12:32:25.916939",
     "start_time": "2017-06-07T16:32:44.269Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame.save('US3M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with HTML and APIs P173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Still needs to dig deeper into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interacting with Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "nav_menu": {
   "height": "134px",
   "width": "327px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "570px",
   "left": "0px",
   "right": "1154px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
