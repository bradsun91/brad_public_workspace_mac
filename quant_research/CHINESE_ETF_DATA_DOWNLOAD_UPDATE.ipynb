{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "# import yfinance as yf\n",
    "import tushare as ts\n",
    "import time, urllib\n",
    "ts.set_token('2f31c3932ead9fcc3830879132cc3ec8df3566550f711889d4a30f67')\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_etfs_df = pd.read_csv(\"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/\"+\"CH_ETFs.csv\")\n",
    "ch_etfs_df.dropna(inplace = True)\n",
    "ch_etfs_df['基金规模\\n[单位] 元'] = ch_etfs_df['基金规模\\n[单位] 元'].apply(lambda x: float(x.replace(\",\",\"\")))\n",
    "ch_etfs_df['机构投资者持有份额\\n[报告期] 2019中报\\n[单位] 份'] = ch_etfs_df['机构投资者持有份额\\n[报告期] 2019中报\\n[单位] 份'].apply(lambda x: float(x.replace(\",\",\"\")))\n",
    "ch_etfs_df.sort_values(\"基金规模\\n[单位] 元\", ascending=False, inplace =True)\n",
    "ch_etfs_df = ch_etfs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>证券代码</th>\n",
       "      <th>证券简称</th>\n",
       "      <th>投资类型(一级分类)</th>\n",
       "      <th>投资类型(二级分类)</th>\n",
       "      <th>基金规模\\n[单位] 元</th>\n",
       "      <th>机构投资者持有份额\\n[报告期] 2019中报\\n[单位] 份</th>\n",
       "      <th>机构投资者持有比例\\n[报告期] 2019中报\\n[单位] %</th>\n",
       "      <th>管理费率\\n[单位] %</th>\n",
       "      <th>托管费率\\n[单位] %</th>\n",
       "      <th>认购费率\\n[收费类型] 前端</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>510500.OF</td>\n",
       "      <td>南方中证500ETF</td>\n",
       "      <td>股票型基金</td>\n",
       "      <td>被动指数型基金</td>\n",
       "      <td>4.349555e+10</td>\n",
       "      <td>6.869517e+09</td>\n",
       "      <td>76.03</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500万份以上 1000元/笔\\n100万份以下 1%\\n100~300万份 0.6%\\n3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>510050.OF</td>\n",
       "      <td>华夏上证50ETF</td>\n",
       "      <td>股票型基金</td>\n",
       "      <td>被动指数型基金</td>\n",
       "      <td>4.166154e+10</td>\n",
       "      <td>1.220778e+10</td>\n",
       "      <td>73.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100万份以下 1%\\n100~1000万份 0.8%\\n1000万份以上 0.5%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>510300.OF</td>\n",
       "      <td>华泰柏瑞沪深300ETF</td>\n",
       "      <td>股票型基金</td>\n",
       "      <td>被动指数型基金</td>\n",
       "      <td>3.451196e+10</td>\n",
       "      <td>6.917064e+09</td>\n",
       "      <td>73.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100万份以上 1000元/笔\\n50万份以下 1%\\n50~100万份 0.5%\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          证券代码          证券简称 投资类型(一级分类) 投资类型(二级分类)  基金规模\\n[单位] 元  \\\n",
       "152  510500.OF    南方中证500ETF      股票型基金    被动指数型基金  4.349555e+10   \n",
       "180  510050.OF     华夏上证50ETF      股票型基金    被动指数型基金  4.166154e+10   \n",
       "162  510300.OF  华泰柏瑞沪深300ETF      股票型基金    被动指数型基金  3.451196e+10   \n",
       "\n",
       "     机构投资者持有份额\\n[报告期] 2019中报\\n[单位] 份  机构投资者持有比例\\n[报告期] 2019中报\\n[单位] %  \\\n",
       "152                     6.869517e+09                            76.03   \n",
       "180                     1.220778e+10                            73.10   \n",
       "162                     6.917064e+09                            73.10   \n",
       "\n",
       "     管理费率\\n[单位] %  托管费率\\n[单位] %  \\\n",
       "152           0.5           0.1   \n",
       "180           0.5           0.1   \n",
       "162           0.5           0.1   \n",
       "\n",
       "                                       认购费率\\n[收费类型] 前端  \n",
       "152  500万份以上 1000元/笔\\n100万份以下 1%\\n100~300万份 0.6%\\n3...  \n",
       "180       100万份以下 1%\\n100~1000万份 0.8%\\n1000万份以上 0.5%\\n  \n",
       "162        100万份以上 1000元/笔\\n50万份以下 1%\\n50~100万份 0.5%\\n  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_etfs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_etfs = ch_etfs_df.copy()\n",
    "ch_etfs['code'] =ch_etfs['证券代码'].apply(lambda x: str(x)[:6])\n",
    "ch_etfs_ticker = list(ch_etfs['code'].unique())\n",
    "# Add the sp500 etf\n",
    "etf_tickers = ['513500']+ch_etfs_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHistoryData(Code, BeginTime, EndTime):\n",
    "    df = ts.get_k_data(Code, index = False,  start = BeginTime, end = EndTime)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['513500', '510500', '510050', '510300', '510330']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_tickers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513500 is new, start downloading now...\n",
      "513500 data file created: 2020-01-02\n",
      "=======================Executed: 1=======================\n",
      "510500 is new, start downloading now...\n",
      "510500 data file created: 2020-01-02\n",
      "=======================Executed: 2=======================\n",
      "510050 is new, start downloading now...\n",
      "510050 data file created: 2020-01-02\n",
      "=======================Executed: 3=======================\n",
      "510300 is new, start downloading now...\n",
      "510300 data file created: 2020-01-02\n",
      "=======================Executed: 4=======================\n",
      "510330 is new, start downloading now...\n",
      "510330 data file created: 2020-01-02\n",
      "=======================Executed: 5=======================\n",
      "159919 is new, start downloading now...\n",
      "159919 data file created: 2020-01-02\n",
      "=======================Executed: 6=======================\n",
      "510180 is new, start downloading now...\n",
      "510180 data file created: 2020-01-02\n",
      "=======================Executed: 7=======================\n",
      "159915 is new, start downloading now...\n",
      "159915 data file created: 2020-01-02\n",
      "=======================Executed: 8=======================\n",
      "512960 is new, start downloading now...\n",
      "512960 data file created: 2020-01-02\n",
      "=======================Executed: 9=======================\n",
      "512950 is new, start downloading now...\n",
      "512950 data file created: 2020-01-02\n",
      "=======================Executed: 10=======================\n",
      "510810 is new, start downloading now...\n",
      "510810 data file created: 2020-01-02\n",
      "=======================Executed: 11=======================\n",
      "512880 is new, start downloading now...\n",
      "512880 data file created: 2020-01-02\n",
      "=======================Executed: 12=======================\n",
      "510900 is new, start downloading now...\n",
      "510900 data file created: 2020-01-02\n",
      "=======================Executed: 13=======================\n",
      "518880 is new, start downloading now...\n",
      "518880 data file created: 2020-01-02\n",
      "=======================Executed: 14=======================\n",
      "510310 is new, start downloading now...\n",
      "510310 data file created: 2020-01-02\n",
      "=======================Executed: 15=======================\n",
      "159901 is new, start downloading now...\n",
      "159901 data file created: 2020-01-02\n",
      "=======================Executed: 16=======================\n",
      "512500 is new, start downloading now...\n",
      "512500 data file created: 2020-01-02\n",
      "=======================Executed: 17=======================\n",
      "159920 is new, start downloading now...\n",
      "159920 data file created: 2020-01-02\n",
      "=======================Executed: 18=======================\n",
      "159949 is new, start downloading now...\n",
      "159949 data file created: 2020-01-02\n",
      "=======================Executed: 19=======================\n",
      "510230 is new, start downloading now...\n",
      "510230 data file created: 2020-01-02\n",
      "=======================Executed: 20=======================\n",
      "510850 is new, start downloading now...\n",
      "510850 data file created: 2020-01-02\n",
      "=======================Executed: 21=======================\n",
      "159959 is new, start downloading now...\n",
      "159959 data file created: 2020-01-02\n",
      "=======================Executed: 22=======================\n",
      "511030 is new, start downloading now...\n",
      "511030 data file created: 2020-01-02\n",
      "=======================Executed: 23=======================\n",
      "510390 is new, start downloading now...\n",
      "510390 data file created: 2020-01-02\n",
      "=======================Executed: 24=======================\n",
      "510510 is new, start downloading now...\n",
      "510510 data file created: 2020-01-02\n",
      "=======================Executed: 25=======================\n",
      "510380 is new, start downloading now...\n",
      "510380 data file created: 2020-01-02\n",
      "=======================Executed: 26=======================\n",
      "512000 is new, start downloading now...\n",
      "512000 data file created: 2020-01-02\n",
      "=======================Executed: 27=======================\n",
      "159962 is new, start downloading now...\n",
      "159962 data file created: 2020-01-02\n",
      "=======================Executed: 28=======================\n",
      "510360 is new, start downloading now...\n",
      "510360 data file created: 2020-01-02\n",
      "=======================Executed: 29=======================\n",
      "159928 is new, start downloading now...\n",
      "159928 data file created: 2020-01-02\n",
      "=======================Executed: 30=======================\n",
      "159922 is new, start downloading now...\n",
      "159922 data file created: 2020-01-02\n",
      "=======================Executed: 31=======================\n",
      "510880 is new, start downloading now...\n",
      "510880 data file created: 2020-01-02\n",
      "=======================Executed: 32=======================\n",
      "159938 is new, start downloading now...\n",
      "159938 data file created: 2020-01-02\n",
      "=======================Executed: 33=======================\n",
      "510590 is new, start downloading now...\n",
      "510590 data file created: 2020-01-02\n",
      "=======================Executed: 34=======================\n",
      "159902 is new, start downloading now...\n",
      "159902 data file created: 2020-01-02\n",
      "=======================Executed: 35=======================\n",
      "512660 is new, start downloading now...\n",
      "512660 data file created: 2020-01-02\n",
      "=======================Executed: 36=======================\n",
      "512070 is new, start downloading now...\n",
      "512070 data file created: 2020-01-02\n",
      "=======================Executed: 37=======================\n",
      "159939 is new, start downloading now...\n",
      "159939 data file created: 2020-01-02\n",
      "=======================Executed: 38=======================\n",
      "511220 is new, start downloading now...\n",
      "511220 data file created: 2020-01-02\n",
      "=======================Executed: 39=======================\n",
      "512900 is new, start downloading now...\n",
      "512900 data file created: 2020-01-02\n",
      "=======================Executed: 40=======================\n",
      "512380 is new, start downloading now...\n",
      "512380 data file created: 2020-01-02\n",
      "=======================Executed: 41=======================\n",
      "512800 is new, start downloading now...\n",
      "512800 data file created: 2020-01-02\n",
      "=======================Executed: 42=======================\n",
      "512580 is new, start downloading now...\n",
      "512580 data file created: 2020-01-02\n",
      "=======================Executed: 43=======================\n",
      "159905 is new, start downloading now...\n",
      "159905 data file created: 2020-01-02\n",
      "=======================Executed: 44=======================\n",
      "159952 is new, start downloading now...\n",
      "159952 data file created: 2020-01-02\n",
      "=======================Executed: 45=======================\n",
      "159910 is new, start downloading now...\n",
      "159910 data file created: 2020-01-02\n",
      "=======================Executed: 46=======================\n",
      "511270 is new, start downloading now...\n",
      "511270 data file created: 2020-01-02\n",
      "=======================Executed: 47=======================\n",
      "513050 is new, start downloading now...\n",
      "513050 data file created: 2020-01-02\n",
      "=======================Executed: 48=======================\n",
      "159948 is new, start downloading now...\n",
      "159948 data file created: 2020-01-02\n",
      "=======================Executed: 49=======================\n",
      "512980 is new, start downloading now...\n",
      "512980 data file created: 2020-01-02\n",
      "=======================Executed: 50=======================\n",
      "511020 is new, start downloading now...\n",
      "511020 data file created: 2020-01-02\n",
      "=======================Executed: 51=======================\n",
      "Already have data csv for 513500\n",
      "Needs to update, start updating new data for 513500 now...\n",
      "New data updated till today for 513500!\n",
      "=======================Executed: 52=======================\n",
      "510580 is new, start downloading now...\n",
      "510580 data file created: 2020-01-02\n",
      "=======================Executed: 53=======================\n",
      "513660 is new, start downloading now...\n",
      "513660 data file created: 2020-01-02\n",
      "=======================Executed: 54=======================\n",
      "512330 is new, start downloading now...\n",
      "512330 data file created: 2020-01-02\n",
      "=======================Executed: 55=======================\n",
      "512010 is new, start downloading now...\n",
      "512010 data file created: 2020-01-02\n",
      "=======================Executed: 56=======================\n",
      "159916 is new, start downloading now...\n",
      "159916 data file created: 2020-01-02\n",
      "=======================Executed: 57=======================\n",
      "512160 is new, start downloading now...\n",
      "512160 data file created: 2020-01-02\n",
      "=======================Executed: 58=======================\n",
      "510560 is new, start downloading now...\n",
      "510560 data file created: 2020-01-02\n",
      "=======================Executed: 59=======================\n",
      "512510 is new, start downloading now...\n",
      "512510 data file created: 2020-01-02\n",
      "=======================Executed: 60=======================\n",
      "513100 is new, start downloading now...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513100 data file created: 2020-01-02\n",
      "=======================Executed: 61=======================\n",
      "512090 is new, start downloading now...\n",
      "512090 data file created: 2020-01-02\n",
      "=======================Executed: 62=======================\n",
      "159961 is new, start downloading now...\n",
      "159961 data file created: 2020-01-02\n",
      "=======================Executed: 63=======================\n",
      "512040 is new, start downloading now...\n",
      "512040 data file created: 2020-01-02\n",
      "=======================Executed: 64=======================\n",
      "512990 is new, start downloading now...\n",
      "512990 data file created: 2020-01-02\n",
      "=======================Executed: 65=======================\n",
      "510160 is new, start downloading now...\n",
      "510160 data file created: 2020-01-02\n",
      "=======================Executed: 66=======================\n",
      "510710 is new, start downloading now...\n",
      "510710 data file created: 2020-01-02\n",
      "=======================Executed: 67=======================\n",
      "512180 is new, start downloading now...\n",
      "512180 data file created: 2020-01-02\n",
      "=======================Executed: 68=======================\n",
      "159960 is new, start downloading now...\n",
      "159960 data file created: 2020-01-02\n",
      "=======================Executed: 69=======================\n",
      "513030 is new, start downloading now...\n",
      "513030 data file created: 2020-01-02\n",
      "=======================Executed: 70=======================\n",
      "511010 is new, start downloading now...\n",
      "511010 data file created: 2020-01-02\n",
      "=======================Executed: 71=======================\n",
      "159940 is new, start downloading now...\n",
      "159940 data file created: 2020-01-02\n",
      "=======================Executed: 72=======================\n",
      "512680 is new, start downloading now...\n",
      "512680 data file created: 2020-01-02\n",
      "=======================Executed: 73=======================\n",
      "512520 is new, start downloading now...\n",
      "512520 data file created: 2020-01-02\n",
      "=======================Executed: 74=======================\n",
      "512280 is new, start downloading now...\n",
      "512280 data file created: 2020-01-02\n",
      "=======================Executed: 75=======================\n",
      "159903 is new, start downloading now...\n",
      "159903 data file created: 2020-01-02\n",
      "=======================Executed: 76=======================\n",
      "512290 is new, start downloading now...\n",
      "512290 data file created: 2020-01-02\n",
      "=======================Executed: 77=======================\n",
      "159933 is new, start downloading now...\n",
      "159933 data file created: 2020-01-02\n",
      "=======================Executed: 78=======================\n",
      "510800 is new, start downloading now...\n",
      "510800 data file created: 2020-01-02\n",
      "=======================Executed: 79=======================\n",
      "510010 is new, start downloading now...\n",
      "510010 data file created: 2020-01-02\n",
      "=======================Executed: 80=======================\n",
      "159929 is new, start downloading now...\n",
      "159929 data file created: 2020-01-02\n",
      "=======================Executed: 81=======================\n",
      "512220 is new, start downloading now...\n",
      "512220 data file created: 2020-01-02\n",
      "=======================Executed: 82=======================\n",
      "512400 is new, start downloading now...\n",
      "512400 data file created: 2020-01-02\n",
      "=======================Executed: 83=======================\n",
      "159935 is new, start downloading now...\n",
      "159935 data file created: 2020-01-02\n",
      "=======================Executed: 84=======================\n",
      "159936 is new, start downloading now...\n",
      "159936 data file created: 2020-01-02\n",
      "=======================Executed: 85=======================\n",
      "512890 is new, start downloading now...\n",
      "512890 data file created: 2020-01-02\n",
      "=======================Executed: 86=======================\n",
      "159909 is new, start downloading now...\n",
      "159909 data file created: 2020-01-02\n",
      "=======================Executed: 87=======================\n",
      "510210 is new, start downloading now...\n",
      "510210 data file created: 2020-01-02\n",
      "=======================Executed: 88=======================\n",
      "512770 is new, start downloading now...\n",
      "512770 data file created: 2020-01-02\n",
      "=======================Executed: 89=======================\n",
      "512700 is new, start downloading now...\n",
      "512700 data file created: 2020-01-02\n",
      "=======================Executed: 90=======================\n",
      "159958 is new, start downloading now...\n",
      "159958 data file created: 2020-01-02\n",
      "=======================Executed: 91=======================\n",
      "159965 is new, start downloading now...\n",
      "159965 data file created: 2020-01-02\n",
      "=======================Executed: 92=======================\n",
      "510130 is new, start downloading now...\n",
      "510130 data file created: 2020-01-02\n",
      "=======================Executed: 93=======================\n",
      "512920 is new, start downloading now...\n",
      "512920 data file created: 2020-01-02\n",
      "=======================Executed: 94=======================\n",
      "159943 is new, start downloading now...\n",
      "159943 data file created: 2020-01-02\n",
      "=======================Executed: 95=======================\n",
      "510630 is new, start downloading now...\n",
      "510630 data file created: 2020-01-02\n",
      "=======================Executed: 96=======================\n",
      "510150 is new, start downloading now...\n",
      "510150 data file created: 2020-01-02\n",
      "=======================Executed: 97=======================\n",
      "512260 is new, start downloading now...\n",
      "512260 data file created: 2020-01-02\n",
      "=======================Executed: 98=======================\n",
      "512390 is new, start downloading now...\n",
      "512390 data file created: 2020-01-02\n",
      "=======================Executed: 99=======================\n",
      "512820 is new, start downloading now...\n",
      "512820 data file created: 2020-01-02\n",
      "=======================Executed: 100=======================\n",
      "159907 is new, start downloading now...\n",
      "159907 data file created: 2020-01-02\n",
      "=======================Executed: 101=======================\n",
      "512360 is new, start downloading now...\n",
      "512360 data file created: 2020-01-02\n",
      "=======================Executed: 102=======================\n",
      "510290 is new, start downloading now...\n",
      "510290 data file created: 2020-01-02\n",
      "=======================Executed: 103=======================\n",
      "512100 is new, start downloading now...\n",
      "512100 data file created: 2020-01-02\n",
      "=======================Executed: 104=======================\n",
      "510020 is new, start downloading now...\n",
      "510020 data file created: 2020-01-02\n",
      "=======================Executed: 105=======================\n",
      "159957 is new, start downloading now...\n",
      "159957 data file created: 2020-01-02\n",
      "=======================Executed: 106=======================\n",
      "512150 is new, start downloading now...\n",
      "512150 data file created: 2020-01-02\n",
      "=======================Executed: 107=======================\n",
      "512550 is new, start downloading now...\n",
      "512550 data file created: 2020-01-02\n",
      "=======================Executed: 108=======================\n",
      "510030 is new, start downloading now...\n",
      "510030 data file created: 2020-01-02\n",
      "=======================Executed: 109=======================\n",
      "510550 is new, start downloading now...\n",
      "510550 data file created: 2020-01-02\n",
      "=======================Executed: 110=======================\n",
      "510680 is new, start downloading now...\n",
      "510680 data file created: 2020-01-02\n",
      "=======================Executed: 111=======================\n",
      "510060 is new, start downloading now...\n",
      "510060 data file created: 2020-01-02\n",
      "=======================Executed: 112=======================\n",
      "512690 is new, start downloading now...\n",
      "512690 data file created: 2020-01-02\n",
      "=======================Executed: 113=======================\n",
      "510220 is new, start downloading now...\n",
      "510220 data file created: 2020-01-02\n",
      "=======================Executed: 114=======================\n",
      "159954 is new, start downloading now...\n",
      "159954 data file created: 2020-01-02\n",
      "=======================Executed: 115=======================\n",
      "159906 is new, start downloading now...\n",
      "159906 data file created: 2020-01-02\n",
      "=======================Executed: 116=======================\n",
      "159918 is new, start downloading now...\n",
      "159918 data file created: 2020-01-02\n",
      "=======================Executed: 117=======================\n",
      "159912 is new, start downloading now...\n",
      "159912 data file created: 2020-01-02\n",
      "=======================Executed: 118=======================\n",
      "510090 is new, start downloading now...\n",
      "510090 data file created: 2020-01-02\n",
      "=======================Executed: 119=======================\n",
      "510170 is new, start downloading now...\n",
      "510170 data file created: 2020-01-02\n",
      "=======================Executed: 120=======================\n",
      "159908 is new, start downloading now...\n",
      "159908 data file created: 2020-01-02\n",
      "=======================Executed: 121=======================\n",
      "512870 is new, start downloading now...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512870 data file created: 2020-01-02\n",
      "=======================Executed: 122=======================\n",
      "512270 is new, start downloading now...\n",
      "512270 data file created: 2020-01-02\n",
      "=======================Executed: 123=======================\n",
      "510190 is new, start downloading now...\n",
      "510190 data file created: 2020-01-02\n",
      "=======================Executed: 124=======================\n",
      "510410 is new, start downloading now...\n",
      "510410 data file created: 2020-01-02\n",
      "=======================Executed: 125=======================\n",
      "510430 is new, start downloading now...\n",
      "510430 data file created: 2020-01-02\n",
      "=======================Executed: 126=======================\n",
      "512120 is new, start downloading now...\n",
      "512120 data file created: 2020-01-02\n",
      "=======================Executed: 127=======================\n",
      "510660 is new, start downloading now...\n",
      "510660 data file created: 2020-01-02\n",
      "=======================Executed: 128=======================\n",
      "510600 is new, start downloading now...\n",
      "510600 data file created: 2020-01-02\n",
      "=======================Executed: 129=======================\n",
      "513600 is new, start downloading now...\n",
      "513600 data file created: 2020-01-02\n",
      "=======================Executed: 130=======================\n",
      "159963 is new, start downloading now...\n",
      "159963 data file created: 2020-01-02\n",
      "=======================Executed: 131=======================\n",
      "510070 is new, start downloading now...\n",
      "510070 data file created: 2020-01-02\n",
      "=======================Executed: 132=======================\n",
      "159913 is new, start downloading now...\n",
      "159913 data file created: 2020-01-02\n",
      "=======================Executed: 133=======================\n",
      "512200 is new, start downloading now...\n",
      "512200 data file created: 2020-01-02\n",
      "=======================Executed: 134=======================\n",
      "512780 is new, start downloading now...\n",
      "512780 data file created: 2020-01-02\n",
      "=======================Executed: 135=======================\n",
      "159956 is new, start downloading now...\n",
      "159956 data file created: 2020-01-02\n",
      "=======================Executed: 136=======================\n",
      "512810 is new, start downloading now...\n",
      "512810 data file created: 2020-01-02\n",
      "=======================Executed: 137=======================\n",
      "512640 is new, start downloading now...\n",
      "512640 data file created: 2020-01-02\n",
      "=======================Executed: 138=======================\n",
      "159964 is new, start downloading now...\n",
      "159964 data file created: 2020-01-02\n",
      "=======================Executed: 139=======================\n",
      "159911 is new, start downloading now...\n",
      "159911 data file created: 2020-01-02\n",
      "=======================Executed: 140=======================\n",
      "512310 is new, start downloading now...\n",
      "512310 data file created: 2020-01-02\n",
      "=======================Executed: 141=======================\n",
      "511280 is new, start downloading now...\n",
      "511280 data file created: 2020-01-02\n",
      "=======================Executed: 142=======================\n",
      "512300 is new, start downloading now...\n",
      "512300 data file created: 2020-01-02\n",
      "=======================Executed: 143=======================\n",
      "513900 is new, start downloading now...\n",
      "513900 data file created: 2020-01-02\n",
      "=======================Executed: 144=======================\n",
      "512340 is new, start downloading now...\n",
      "512340 data file created: 2020-01-02\n",
      "=======================Executed: 145=======================\n",
      "510650 is new, start downloading now...\n",
      "510650 data file created: 2020-01-02\n",
      "=======================Executed: 146=======================\n",
      "511310 is new, start downloading now...\n",
      "511310 data file created: 2020-01-02\n",
      "=======================Executed: 147=======================\n",
      "510110 is new, start downloading now...\n",
      "510110 data file created: 2020-01-02\n",
      "=======================Executed: 148=======================\n",
      "512860 is new, start downloading now...\n",
      "512860 data file created: 2020-01-02\n",
      "=======================Executed: 149=======================\n",
      "510440 is new, start downloading now...\n",
      "510440 data file created: 2020-01-02\n",
      "=======================Executed: 150=======================\n",
      "159923 is new, start downloading now...\n",
      "159923 data file created: 2020-01-02\n",
      "=======================Executed: 151=======================\n",
      "159931 is new, start downloading now...\n",
      "159931 data file created: 2020-01-02\n",
      "=======================Executed: 152=======================\n",
      "159932 is new, start downloading now...\n",
      "159932 data file created: 2020-01-02\n",
      "=======================Executed: 153=======================\n",
      "159945 is new, start downloading now...\n",
      "159945 data file created: 2020-01-02\n",
      "=======================Executed: 154=======================\n",
      "511290 is new, start downloading now...\n",
      "511290 data file created: 2020-01-02\n",
      "=======================Executed: 155=======================\n",
      "159955 is new, start downloading now...\n",
      "159955 data file created: 2020-01-02\n",
      "=======================Executed: 156=======================\n",
      "512850 is new, start downloading now...\n",
      "512850 data file created: 2020-01-02\n",
      "=======================Executed: 157=======================\n",
      "512560 is new, start downloading now...\n",
      "512560 data file created: 2020-01-02\n",
      "=======================Executed: 158=======================\n",
      "510120 is new, start downloading now...\n",
      "510120 data file created: 2020-01-02\n",
      "=======================Executed: 159=======================\n",
      "512590 is new, start downloading now...\n",
      "512590 data file created: 2020-01-02\n",
      "=======================Executed: 160=======================\n",
      "159944 is new, start downloading now...\n",
      "159944 data file created: 2020-01-02\n",
      "=======================Executed: 161=======================\n",
      "159953 is new, start downloading now...\n",
      "159953 data file created: 2020-01-02\n",
      "=======================Executed: 162=======================\n",
      "159941 is new, start downloading now...\n",
      "159941 data file created: 2020-01-02\n",
      "=======================Executed: 163=======================\n",
      "159930 is new, start downloading now...\n",
      "159930 data file created: 2020-01-02\n",
      "=======================Executed: 164=======================\n",
      "512600 is new, start downloading now...\n",
      "512600 data file created: 2020-01-02\n",
      "=======================Executed: 165=======================\n",
      "510270 is new, start downloading now...\n",
      "510270 data file created: 2020-01-02\n",
      "=======================Executed: 166=======================\n",
      "512610 is new, start downloading now...\n",
      "512610 data file created: 2020-01-02\n",
      "=======================Executed: 167=======================\n",
      "159951 is new, start downloading now...\n",
      "159951 data file created: 2020-01-02\n",
      "=======================Executed: 168=======================\n",
      "159926 is new, start downloading now...\n",
      "159926 data file created: 2020-01-02\n",
      "=======================Executed: 169=======================\n",
      "513680 is new, start downloading now...\n",
      "513680 data file created: 2020-01-02\n",
      "=======================Executed: 170=======================\n",
      "【Updated Finished for today!】\n"
     ]
    }
   ],
   "source": [
    "import eventlet\n",
    "eventlet.monkey_patch()\n",
    "\n",
    "ch_db_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/CH_database/\"\n",
    "ticker_list = etf_tickers\n",
    "# ticker_list = ['511020']\n",
    "# ticker_list = ['513500',#标普500\n",
    "#                '501021',#香港中小\n",
    "#                # 目前黄金ETF基金主要有四只，易方达黄金ETF(159934)、博时黄金159937、国泰黄金ETF（518800）、华安黄金ETF（518880）。\n",
    "#                '159934','159937','518800','518880'\n",
    "#                # 300ETF（510300）、创业板ETF（159915）、500ETF（510500）都是指数ETF基金。\n",
    "#                # 除了这些宽基指数基金外，还有跟踪行业指数的指数ETF，比如金融ETF(510230)、银行ETF（512800）、\n",
    "#                # 军工ETF（512660）、广发医药（159938）等。\n",
    "#                '510300','159915','510500','510230','512800','512660','159938']\n",
    "today = str(datetime.now().date())\n",
    "start = '2010-01-01'\n",
    "end = today\n",
    "count = 1\n",
    "for ticker in ticker_list:\n",
    "    if count%200==0:\n",
    "        print(\"=======================Sleeping======================\")\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        if not os.path.exists(ch_db_path+ticker+\".csv\"):\n",
    "            print(\"{} is new, start downloading now...\".format(ticker))\n",
    "            try:\n",
    "                ############ replace get_data with other customized functions #############\n",
    "                data = GetHistoryData(ticker, start, today)\n",
    "                ############ replace get_data with other customized functions #############\n",
    "#                 print(data)\n",
    "                data['date'] = data['date'].astype(str)\n",
    "#                 data['date'] = data['date'].apply(lambda x:x[:4]+\"-\"+x[4:6]+\"-\"+x[6:] if len(x)!=10 else x)\n",
    "                data.sort_values(\"date\", inplace = True)\n",
    "                data.to_csv(ch_db_path+ticker+\".csv\", index = False)\n",
    "                print(\"{} data file created: {}\".format(ticker, end))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        else:\n",
    "            print(\"Already have data csv for {}\".format(ticker))\n",
    "            hist_data = pd.read_csv(ch_db_path+ticker+\".csv\")   \n",
    "            hist_data['date'] = hist_data['date'].astype(str)\n",
    "#             hist_data['date'] = hist_data['date'].apply(lambda x:x[:4]+\"-\"+x[4:6]+\"-\"+x[6:] if len(x)!=10 else x)\n",
    "            hist_data.to_csv(ch_db_path+ticker+\".csv\", index = False)\n",
    "            hist_data = pd.read_csv(ch_db_path+ticker+\".csv\")\n",
    "            try:\n",
    "                hist_data_last_date = hist_data['date'].values[-1]        \n",
    "                if today > hist_data_last_date:\n",
    "                    print(\"Needs to update, start updating new data for {} now...\".format(ticker))\n",
    "                    update_start = hist_data_last_date\n",
    "                    update_end = today\n",
    "                    with eventlet.Timeout(60,False):\n",
    "                        try:\n",
    "                            ############ replace get_data with other customized functions #############\n",
    "                            new_data = GetHistoryData(ticker, update_start, update_end)\n",
    "                            ############ replace get_data with other customized functions #############\n",
    "                            \n",
    "                            new_data['date'] = new_data['date'].astype(str)\n",
    "                            new_data['date'] = new_data['date'].apply(lambda x:x[:4]+\"-\"+x[4:6]+\"-\"+x[6:])\n",
    "                            new_data.to_csv(ch_db_path+ticker+\".csv\", mode='a', header=False, index = False)\n",
    "                            updated_duplicated_df = pd.read_csv(ch_db_path+ticker+\".csv\")\n",
    "                            updated_df = updated_duplicated_df.drop_duplicates(\"date\")\n",
    "                            updated_df.sort_values(\"date\", inplace = True)\n",
    "                            updated_df.to_csv(ch_db_path+ticker+\".csv\", index = False)\n",
    "                            print(\"New data updated till today for {}!\".format(ticker))\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "        #             print(\"Timed Out: Update Failed!\")\n",
    "                else:\n",
    "                    print(\"There's no new data to update for {}.\".format(ticker))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    #     print(\"Data Download/Update for {} is Finished.\".format(ticker))\n",
    "        print(\"=======================Executed: {}=======================\".format(count))\n",
    "    count+=1\n",
    "print(\"【Updated Finished for today!】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
